---
layout: base
title: Codebase Overview
---

# ðŸ“‚ Codebase Overview
{: .no_toc }

This page provides a comprehensive guide to the HipGPT codebase architecture and explains the purpose of each component. The project is designed to be **self-contained**, **readable**, and **educational**, allowing you to understand how a GPT-style transformer works from the ground up.

## Table of Contents
{: .no_toc .text-delta }

1. TOC
{:toc}

---

## Project Structure

```
hipgpt/
â”œâ”€â”€ ðŸ“ build/                 # Build outputs (CMake artifacts)
â”œâ”€â”€ ðŸ“ data/                  # Training datasets
â”‚   â””â”€â”€ tiny_shakespeare.txt
â”œâ”€â”€ ðŸ“ scripts/               # Automation scripts
â”‚   â”œâ”€â”€ download_data.sh      # Dataset fetching
â”‚   â””â”€â”€ run_train.sh          # End-to-end training
â”œâ”€â”€ ðŸ“ include/               # Public headers
â”‚   â”œâ”€â”€ gpt_model.h           # Main model interface
â”‚   â”œâ”€â”€ hip_kernels.h         # GPU kernel declarations
â”‚   â”œâ”€â”€ tokenizer.h           # BPE tokenizer interface
â”‚   â””â”€â”€ transformer_layer.h   # Transformer block interface
â”œâ”€â”€ ðŸ“ src/                   # Implementation files
â”‚   â”œâ”€â”€ generate.cpp          # Text generation CLI
â”‚   â”œâ”€â”€ gpt_model.cpp         # Model orchestration
â”‚   â”œâ”€â”€ hip_kernels.cpp       # GPU kernel implementations
â”‚   â”œâ”€â”€ tokenizer.cpp         # BPE tokenizer logic
â”‚   â”œâ”€â”€ train_gpt.cpp         # Training CLI
â”‚   â””â”€â”€ transformer_layer.cpp # Transformer block logic
â”œâ”€â”€ CMakeLists.txt            # Build configuration
â”œâ”€â”€ LICENSE                   # MIT License
â””â”€â”€ README.md                 # Project documentation
```

---

## Core Components

### ðŸ”¤ Tokenizer
{: .d-inline-block }
Essential
{: .label .label-blue }

**Files:** `src/tokenizer.cpp`, `include/tokenizer.h`

The tokenizer implements Byte-Pair Encoding (BPE) from scratch, handling the conversion between human-readable text and model-compatible token sequences.

**Key Responsibilities:**
- **Vocabulary Training** â€” Learns subword tokens from training corpus
- **Text Encoding** â€” Converts strings to token ID sequences  
- **Text Decoding** â€” Reconstructs text from token sequences
- **Serialization** â€” Saves/loads trained vocabulary using JSON

**Usage Example:**
```cpp
Tokenizer tokenizer;
tokenizer.train("data/tiny_shakespeare.txt", vocab_size=1000);
auto tokens = tokenizer.encode("Hello, world!");
std::string text = tokenizer.decode(tokens);
```

---

### ðŸ§  Transformer Layer
{: .d-inline-block }
Core
{: .label .label-green }

**Files:** `src/transformer_layer.cpp`, `include/transformer_layer.h`

Implements a single GPT-style transformer block containing the fundamental attention and feed-forward mechanisms.

**Architecture Components:**
- **Multi-Head Self-Attention** â€” Parallel attention heads with QKV projections
- **Feed-Forward Network** â€” Two-layer MLP with ReLU activation
- **Residual Connections** â€” Skip connections around attention and FFN
- **Layer Normalization** â€” Pre-normalization for stable training
- **Dropout** â€” Regularization during training phase

**Key Features:**
- Each layer manages its own GPU memory and optimizer states
- Supports both training (with gradients) and inference modes  
- Configurable head count, hidden dimensions, and dropout rates

---

### ðŸ—ï¸ GPT Model
{: .d-inline-block }
Architecture
{: .label .label-purple }

**Files:** `src/gpt_model.cpp`, `include/gpt_model.h`

The main model class that orchestrates the complete GPT architecture by combining embeddings, transformer layers, and output projections.

**Model Pipeline:**
1. **Token Embeddings** â€” Maps token IDs to dense vectors
2. **Positional Embeddings** â€” Adds position information  
3. **Transformer Stack** â€” Sequential transformer layers
4. **Output Projection** â€” Linear layer to vocabulary logits
5. **Loss Computation** â€” Cross-entropy for training

**Training Features:**
- Automatic gradient computation and backpropagation
- Adam optimizer with configurable learning rate
- Checkpoint saving and loading
- Memory-efficient batch processing

---

### âš¡ HIP Kernels
{: .d-inline-block }
Performance
{: .label .label-yellow }

**Files:** `src/hip_kernels.cpp`, `include/hip_kernels.h`

Custom GPU kernels implemented in AMD HIP, providing transparent and educational implementations of all neural network operations.

**Implemented Operations:**

| Kernel Category | Operations |
|:----------------|:-----------|
| **Linear Algebra** | Matrix multiplication, bias addition, transpose |
| **Attention** | Multi-head QKV computation, scaled dot-product attention |
| **Activations** | ReLU, Softmax, GELU |
| **Normalization** | Layer normalization, dropout |
| **Training** | Cross-entropy loss, accuracy computation, gradient updates |

**Design Philosophy:**
- **Educational Transparency** â€” No black-box external libraries
- **Performance Optimized** â€” Efficient memory access patterns
- **Numerically Stable** â€” Careful handling of floating-point operations

---

## Application Entry Points

### ðŸŽ¯ Training Pipeline
{: .d-inline-block }
CLI
{: .label .label-red }

**File:** `src/train_gpt.cpp`

Complete training workflow from raw text to trained model.

**Training Process:**
```mermaid
graph LR
    A[Raw Text] --> B[Tokenizer Training]
    B --> C[Dataset Preparation]
    C --> D[Model Initialization]
    D --> E[Training Loop]
    E --> F[Checkpointing]
```

**Command-line Interface:**
```bash
./build/train_gpt \
  --data data/tiny_shakespeare.txt \
  --vocab_size 1000 \
  --seq_length 128 \
  --batch_size 32 \
  --learning_rate 3e-4 \
  --epochs 10
```

---

### ðŸ”® Text Generation
{: .d-inline-block }
CLI
{: .label .label-red }

**File:** `src/generate.cpp`

Interactive text generation interface for trained models.

**Generation Features:**
- **Top-k Sampling** â€” Selects from k most likely tokens
- **Temperature Control** â€” Adjusts randomness in generation
- **Prompt Conditioning** â€” Starts generation from user input
- **Real-time Output** â€” Streams generated text as it's produced

**Usage Example:**
```bash
./build/generate \
  --model checkpoints/model_final.bin \
  --tokenizer checkpoints/tokenizer.json \
  --prompt "To be or not to be" \
  --max_tokens 100 \
  --temperature 0.8
```

---

## Build System & Scripts

### ðŸ”§ CMake Configuration
{: .d-inline-block }
Build
{: .label .label-grey }

**File:** `CMakeLists.txt`

- Configures HIP compilation with `hipcc`
- Automatically fetches nlohmann/json for serialization
- Sets up proper include paths and linking
- Supports both Debug and Release configurations

### ðŸ“œ Helper Scripts
{: .d-inline-block }
Automation
{: .label .label-grey }

**Data Management:**
```bash
# Download training dataset
./scripts/download_data.sh
```

**Training Automation:**
```bash  
# Complete training pipeline
./scripts/run_train.sh [custom_args...]
```

---

## Data Flow Architecture

The following diagram illustrates how data flows through the HipGPT system:

```mermaid
graph TD
    A[Raw Text] --> B[Tokenizer]
    B --> C[Token IDs]
    C --> D[Embeddings Layer]
    D --> E[Transformer Stack]
    E --> F[Layer 1: Self-Attention + FFN]
    F --> G[Layer 2: Self-Attention + FFN]
    G --> H[...]
    H --> I[Layer N: Self-Attention + FFN]
    I --> J[Output Projection]
    J --> K[Vocabulary Logits]
    K --> L[Loss Computation]
    
    style A fill:#e1f5fe
    style K fill:#f3e5f5
    style L fill:#fff3e0
```

---

## Getting Started

1. **Clone and Build:**
   ```bash
   git clone <repository-url>
   cd hipgpt
   mkdir build && cd build
   cmake .. && make -j
   ```

2. **Download Data:**
   ```bash
   ./scripts/download_data.sh
   ```

3. **Train Model:**
   ```bash
   ./scripts/run_train.sh
   ```

4. **Generate Text:**
   ```bash
   ./build/generate --model checkpoints/model_final.bin
   ```

---

## Summary

HipGPT's architecture follows a clean, modular design that makes transformer internals accessible:

- **ðŸ”¤ Tokenizer** â€” Handles text â†” token conversion
- **ðŸ§  TransformerLayer** â€” Implements core attention mechanism  
- **ðŸ—ï¸ GPTModel** â€” Orchestrates the complete architecture
- **âš¡ HIP Kernels** â€” Provides efficient GPU operations
- **ðŸŽ¯ Training/Generation** â€” Delivers end-to-end workflows

This design enables you to trace the complete journey from **raw text â†’ tokens â†’ embeddings â†’ attention â†’ logits â†’ generated text** with full transparency at every step.

{: .highlight }
> **Educational Goal:** Every component is implemented from scratch to maximize learning and understanding of transformer architecture internals.